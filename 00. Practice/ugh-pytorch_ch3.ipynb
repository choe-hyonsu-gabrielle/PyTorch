{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import here\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]]) <class 'torch.Tensor'>\n",
      "tensor([[1, 2],\n",
      "        [3, 4]]) <class 'torch.Tensor'>\n",
      "[[1 2]\n",
      " [3 4]] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Tensors\n",
    "\n",
    "\n",
    "x = torch.Tensor([[1,2],[3,4]])  # 기본값은 *.FloatTensor\n",
    "y = torch.from_numpy(np.array([[1,2],[3,4]]))  # 책의 코드는 import 순서가 잘못됨\n",
    "z = np.array([[1,2],[3,4]])\n",
    "\n",
    "print(x, type(x))\n",
    "print(y, type(y))\n",
    "print(z, type(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[6.3490e+22, 4.5629e-41],\n",
      "        [1.6715e+21, 3.0813e-41]])\n",
      "y = tensor([[6.3490e+22, 4.5629e-41],\n",
      "        [1.6792e+21, 3.0813e-41]])\n",
      "x' = tensor([[6.3490e+22, 4.5629e-41],\n",
      "        [1.6715e+21, 3.0813e-41]])\n",
      "y' = tensor([[6.3490e+22, 4.5629e-41],\n",
      "        [1.6792e+21, 3.0813e-41]], requires_grad=True)\n",
      "z = tensor([[1.2865e+23, 1.2207e-40],\n",
      "        [5.0427e+21, 9.2439e-41]], grad_fn=<AddBackward0>)\n",
      "z' = tensor([[1.2717e+23, 1.2207e-40],\n",
      "        [5.0096e+21, 9.2439e-41]])\n"
     ]
    }
   ],
   "source": [
    "# Autograd\n",
    "\n",
    "\n",
    "x = torch.FloatTensor(2,2)\n",
    "y = torch.FloatTensor(2,2)\n",
    "\n",
    "print('x =', x)\n",
    "print('y =', y)\n",
    "\n",
    "y.requires_grad_(True)\n",
    "\n",
    "z = (x + y) + torch.FloatTensor(2,2)  # 연산 그래프에 할당\n",
    "\n",
    "print('x\\' =', x)\n",
    "print('y\\' =', y)\n",
    "print('z =', z)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = (x + y) + torch.FloatTensor(2,2)\n",
    "    print('z\\' =', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[6.3491e+22, 4.5629e-41, 1.7828e+20, 3.0813e-41, 4.4842e-44],\n",
      "        [0.0000e+00, 1.5695e-43, 0.0000e+00, 1.7828e+20, 3.0813e-41],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 9.1835e-41, 2.3694e-38, 2.3694e-38, 2.3694e-38]])\n",
      "W = tensor([[ 6.3491e+22,  4.5629e-41,  1.8875e+20],\n",
      "        [ 3.0813e-41,  4.4842e-44,  0.0000e+00],\n",
      "        [ 8.9683e-44,  0.0000e+00,  1.6608e+21],\n",
      "        [ 3.0813e-41, -1.8325e+25,  4.5628e-41],\n",
      "        [-1.8326e+25,  4.5628e-41, -1.8325e+25]])\n",
      "torch.mm(x, W) = tensor([[        inf, -5.6177e-16,         inf],\n",
      "        [-5.6466e-16,        -inf, -5.6463e-16],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-4.3421e-13, -4.3421e-13, -4.3415e-13]])\n",
      "b = tensor([6.3490e+22, 4.5629e-41, 1.6589e+21])\n",
      "y = tensor([[        inf, -5.6177e-16,         inf],\n",
      "        [ 6.3490e+22,        -inf,  1.6589e+21],\n",
      "        [ 6.3490e+22,  4.5629e-41,  1.6589e+21],\n",
      "        [ 6.3490e+22, -4.3421e-13,  1.6589e+21]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# Feed-forward\n",
    "\n",
    "\n",
    "def linear(x, W, b):\n",
    "    y = torch.mm(x, W) + b  # Matrix multiplication\n",
    "    return y\n",
    "\n",
    "x = torch.FloatTensor(4,5)  # 책에서는 indentation 잘못됨\n",
    "W = torch.FloatTensor(5,3)\n",
    "b = torch.FloatTensor(3)\n",
    "\n",
    "print('x =', x)\n",
    "print('W =', W)\n",
    "print('torch.mm(x, W) =', torch.mm(x, W))\n",
    "print('b =', b)\n",
    "\n",
    "y = linear(x, W, b)  # (16*10) * (10*5) = (16*5) --> (16*5) + (5,) \n",
    "\n",
    "print('y =', y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[6.3491e+22, 4.5629e-41, 6.3491e+22, 4.5629e-41, 4.4842e-44],\n",
      "        [0.0000e+00, 1.5695e-43, 0.0000e+00, 1.7037e+21, 3.0813e-41],\n",
      "        [1.1495e+24, 3.0881e+29, 2.5226e-18, 4.2330e+21, 1.6534e+19],\n",
      "        [3.0601e+32, 3.3129e-18, 7.2646e+22, 7.2250e+28, 2.5226e-18]])\n",
      "y = tensor([[       inf, 6.2118e-18,        inf],\n",
      "        [1.6901e+21,        inf,        inf],\n",
      "        [       inf,        inf,        inf],\n",
      "        [       inf,        inf,        inf]], grad_fn=<AddBackward0>)\n",
      "params = [torch.Size([5, 3]), torch.Size([3])]\n"
     ]
    }
   ],
   "source": [
    "# nn.Module\n",
    "\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_size, output_size), requires_grad=True)  # 책에서 indentation 잘못됨\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_size), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.mm(x, self.W) + self.b\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# input텐서 선언\n",
    "x = torch.FloatTensor(4, 5)\n",
    "print('x =', x)\n",
    "\n",
    "# 모델 initiating\n",
    "linear = MyLinear(5, 3)  # |x|=(c,r)의 r과 self.W의 input_size가 일치해야 함\n",
    "\n",
    "# feed-forward\n",
    "y = linear(x)\n",
    "\n",
    "print('y =', y)\n",
    "\n",
    "# 학습 가능한 파라미터 확인\n",
    "print('params =', [p.size() for p in linear.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[1.6159e+21, 3.0813e-41, 1.6921e+21, 3.0813e-41, 7.0065e-45],\n",
      "        [0.0000e+00, 5.4889e-05, 1.7063e-07, 1.3147e+22, 2.1651e+23],\n",
      "        [1.3224e-08, 3.3978e+21, 2.0179e-43, 0.0000e+00, 1.3593e-43],\n",
      "        [0.0000e+00, 1.8040e+21, 3.0813e-41, 6.3490e+22, 4.5629e-41]])\n",
      "y = tensor([[-2.6387e+20, -6.6584e+20, -4.4611e+20],\n",
      "        [-6.5335e+22,  4.1922e+22, -1.7814e+22],\n",
      "        [-3.8260e+19,  2.9861e+20, -6.1604e+20],\n",
      "        [ 2.0433e+22, -2.2229e+22,  2.5959e+22]], grad_fn=<AddmmBackward>)\n",
      "params = [torch.Size([3, 5]), torch.Size([3])]\n",
      "linear = MyLinear(\n",
      "  (linear): Linear(in_features=5, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# nn.Module (Prettier version)\n",
    "# c.f. https://hashcode.co.kr/questions/6419/python3-super와-supera-self의-차이는-무엇인가요\n",
    "\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()  # super(MyLinear, self).__init__()과 같은 기능\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# input텐서 선언\n",
    "x = torch.FloatTensor(4, 5)\n",
    "print('x =', x)\n",
    "\n",
    "# 모델 initiating\n",
    "linear = MyLinear(5, 3)  # |x|=(c,r)의 r과 self.W의 input_size가 일치해야 함\n",
    "\n",
    "# feed-forward\n",
    "y = linear(x)\n",
    "print('y =', y)\n",
    "\n",
    "# 학습 가능한 파라미터 확인\n",
    "print('params =', [p.size() for p in linear.parameters()])\n",
    "\n",
    "# linear 확인\n",
    "print('linear =', linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = tensor(nan, grad_fn=<DivBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Back-propagation\n",
    "\n",
    "\n",
    "# 임의로 정한 정답 값을 100이라 하자\n",
    "target = 100\n",
    "\n",
    "# 입력 데이터\n",
    "x = torch.FloatTensor(4,5)\n",
    "\n",
    "# 모델 initiation\n",
    "linear = MyLinear(5, 3)\n",
    "\n",
    "# forward\n",
    "prediction = linear(x)\n",
    "\n",
    "# loss/cost 계산\n",
    "loss = (target - prediction.sum()) / 2   # 두 scalar 값 사이의 거리를 구하는 것으로 loss를 계산할 경우. \n",
    "print('loss =', loss)\n",
    "\n",
    "# target과 prediction을 단일 스칼라 값으로 표현되는 이유?\n",
    "# 그냥 대충 '이만큼' 고칠 것이다를 정하고 lr * grad(loss)만큼 params를 역전파 업데이트하는 것...?\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyLinear(\n",
       "  (linear): Linear(in_features=5, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train() & eval()\n",
    "\n",
    "\n",
    "linear.eval()   # 학습(params 업데이트 및 드롭아웃 등)되지 않음\n",
    "\n",
    "linear.train()   # default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "tensor(-6.1582e-05) tensor(0.9000) tensor(1.2316)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Overall implementation\n",
    "\n",
    "\n",
    "# 1. 임의로 생성한 텐서들을\n",
    "# 2. 근사하고자 하는 정답 함수에 넣어 정답(y)를 구하고,\n",
    "# 3. 그 정답과 신경망을 통과한 y_hat(prediction)과의 차이(error|loss)를 평균제곱오차(MSE)를 통해 구하여\n",
    "# 4. 확률적 경사하강법(SGD)를 통해 최적화\n",
    "\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()   # 부모 클래스의 모든 조건 상속\n",
    "        self.linear = nn.Linear(input_size, output_size)   # <-- nn.Linear는 레이어 딱 한 층\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)   # x = input, y = prediction\n",
    "        return y\n",
    "    \n",
    "    \n",
    "\n",
    "def ground_truth(x):\n",
    "    return 3 * x[:, 0] + x[:, 1] - 2 * x[:, 2]\n",
    "\n",
    "\n",
    "def train(model, x, y, optim):\n",
    "    \n",
    "    # 모든 기울기 초기화\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    # 순전파\n",
    "    prediction = model(x)\n",
    "    \n",
    "    # 실제 정답과 예측된 값의 오차(손실) 계산\n",
    "    loss = ((y - prediction) / 2).sum() / x.size(0)\n",
    "    \n",
    "    # 역전파\n",
    "    loss.backward()\n",
    "    \n",
    "    # 경사 하강 수행\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data\n",
    "\n",
    "\n",
    "# 앞의 함수들을 활용하기 위한 하이퍼파라미터 설정\n",
    "batch_size = 1\n",
    "epochs = 1000\n",
    "iteration = 10000\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "# 모델 및 옵티마이저 선언\n",
    "model = MyModel(3,1)\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.1)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    avg_loss = 0\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        x = torch.rand(batch_size, 3)\n",
    "        y = ground_truth(x.data)\n",
    "        \n",
    "        loss = train(model, x, y, optim)\n",
    "        \n",
    "        avg_loss += loss\n",
    "        avg_loss = avg_loss / iteration\n",
    "        \n",
    "        \n",
    "    # 모델이 정상인지 간단한 테스트\n",
    "    x_valid = torch.FloatTensor([[.3, .2, .1]])\n",
    "    y_valid = ground_truth(x_valid.data)\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    \n",
    "    model.train()   # 다시 원상태로\n",
    "    \n",
    "    print(avg_loss, y_valid.data[0], y_hat.data[0,0])\n",
    "    \n",
    "    \n",
    "    if avg_loss < .0001:\n",
    "        break\n",
    "    else:\n",
    "        print(avg_loss)\n",
    "        \n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
